{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfVRS7wIoMv_"
      },
      "outputs": [],
      "source": [
        "desired_directory = 'Logic-LLM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q59YaGRy0HJa"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qwv0ZMM0Jt0"
      },
      "outputs": [],
      "source": [
        "os.chdir(desired_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Moc_NmDQ63DU"
      },
      "outputs": [],
      "source": [
        "import huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICN1mMkmL3re"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "token = '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'\n",
        "login(token = token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E4f76b1z_7R"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!pip install nltk\n",
        "!pip install z3-solver\n",
        "!pip install ply\n",
        "!pip install func_timeout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imports"
      ],
      "metadata": {
        "id": "aiYEtutX78Qs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC30Bg6iL6nK"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM,LlamaForCausalLM\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration,AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from collections import Counter\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple\n",
        "import nltk\n",
        "# model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# tokenizer=AutoTokenizer.from_pretrained(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0RLnpAiBmt9"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3XqezpLBFu"
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WhXhYRss866"
      },
      "outputs": [],
      "source": [
        "def prompt_LSAT(in_context_example, test_example):\n",
        "        full_prompt = in_context_example\n",
        "        context = test_example['context'].strip()\n",
        "        question = test_example['question'].strip()\n",
        "        options = '\\n'.join([opt.strip() for opt in test_example['options']])\n",
        "        full_prompt = full_prompt.replace('[[CONTEXT]]', context)\n",
        "        full_prompt = full_prompt.replace('[[QUESTION]]', question)\n",
        "        full_prompt = full_prompt.replace('[[OPTIONS]]', options)\n",
        "        return full_prompt\n",
        "\n",
        "def load_in_context_examples(demonstration_path,dataset_name,mode):\n",
        "        with open(os.path.join(demonstration_path, f'{dataset_name}_{mode}.txt')) as f:\n",
        "            in_context_examples = f.read()\n",
        "        return in_context_examples\n",
        "def load_raw_dataset(data_path, dataset_name, split):\n",
        "        with open(os.path.join(data_path, dataset_name, f'{split}.json')) as f:\n",
        "            raw_dataset = json.load(f)\n",
        "        return raw_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demonstration_path=\"baselines/icl_examples/\"\n",
        "dataset_name=\"FOLIO\" # \"AR-LSAT\"  \"LogicalDeduction\"\n",
        "split=\"dev\"\n",
        "mode=\"CoT\"  # \"Direct\"\n",
        "data_path=\"data\""
      ],
      "metadata": {
        "id": "2bWtIFo27ExX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEsDlh1V2yH2"
      },
      "outputs": [],
      "source": [
        "save_path=\"baselines/results\"  # add model name after results - results/llama2 or results/flant5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### for llama2"
      ],
      "metadata": {
        "id": "Bl6CPRzH7W4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTVRSyhw63DY"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# for flan t5"
      ],
      "metadata": {
        "id": "MUpyeo4V7aHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "fF2R84DK7bbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roSVVsQn63DZ"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHz35EKhLD55"
      },
      "source": [
        "### baseline code direct and CoT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3CxVoAeLGUs"
      },
      "outputs": [],
      "source": [
        "for dataset_name in ['FOLIO','AR-LSAT','LogicalDeduction']:\n",
        "    for mode in ['CoT','Direct']:\n",
        "        # load raw dataset\n",
        "        raw_dataset = load_raw_dataset(data_path,dataset_name,split)\n",
        "        print(f\"Loaded {len(raw_dataset)} examples from {split} split.\")\n",
        "\n",
        "        # load in-context examples\n",
        "        in_context_examples = load_in_context_examples(demonstration_path,dataset_name,mode)\n",
        "\n",
        "        outputs_list = []\n",
        "        for example in tqdm(raw_dataset):\n",
        "\n",
        "            print(example)\n",
        "            question = example['question']\n",
        "\n",
        "            # create prompt\n",
        "            full_prompt = prompt_LSAT(in_context_examples, example)\n",
        "            print(\"full prompt:\"f'{full_prompt}')\n",
        "            # change here for the model you want to use\n",
        "            inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            outputs = model.generate(inputs,temperature=0.01,do_sample = True)\n",
        "\n",
        "            output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "            print(output)\n",
        "\n",
        "            # get the answer\n",
        "            label_phrase = 'The correct option is:'\n",
        "            generated_answer = output.split(label_phrase)[-1].strip()\n",
        "            generated_reasoning = output.split(label_phrase)[0].strip()\n",
        "\n",
        "            # create output\n",
        "            output = {'id': example['id'],\n",
        "                        'question': question,\n",
        "                        'answer': example['answer'],\n",
        "                        'predicted_reasoning': generated_reasoning,\n",
        "                        'predicted_answer': generated_answer}\n",
        "            outputs_list.append(output)\n",
        "\n",
        "\n",
        "        # save results\n",
        "        with open(os.path.join(save_path, f'{dataset_name}_{mode}_{split}.json'), 'w') as f:\n",
        "            json.dump(outputs_list, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoD4tUwDjZWf"
      },
      "source": [
        "### logic inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_E7iG14jlnD"
      },
      "outputs": [],
      "source": [
        "def load_prompt_templates(dataset_name):\n",
        "    prompt_file = f'./models/prompts/{dataset_name}.txt'\n",
        "    with open(prompt_file, 'r',encoding='utf-8') as f:\n",
        "        prompt_template = f.read()\n",
        "    return prompt_template\n",
        "\n",
        "def prompt_folio(test_data,prompt_template):\n",
        "        problem = test_data['context']\n",
        "        question = test_data['question'].strip()\n",
        "        full_prompt = prompt_template.replace('[[PROBLEM]]', problem).replace('[[QUESTION]]', question)\n",
        "        return full_prompt\n",
        "\n",
        "def load_raw_dataset(data_path, dataset_name, split):\n",
        "    with open(os.path.join(data_path, dataset_name, f'{split}.json'),encoding='utf-8') as f:\n",
        "        raw_dataset = json.load(f)\n",
        "    return raw_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsNCHZXWkzOo"
      },
      "outputs": [],
      "source": [
        "demonstration_path=\"baselines/icl_examples/\"\n",
        "dataset_name=\"FOLIO\" # \"AR-LSAT\"  \"LogicalDeduction\"\n",
        "split=\"dev\"\n",
        "mode=\"CoT\"\n",
        "data_path=\"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrYkKbbg4B0P"
      },
      "outputs": [],
      "source": [
        "save_path=\"outputs/logic_programs\"            # logic_programs/flant5 or llama2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwbGsKgB3fZs"
      },
      "outputs": [],
      "source": [
        "for dataset_name in ['FOLIO','AR-LSAT','LogicalDeduction']:\n",
        "    # load raw dataset\n",
        "    raw_dataset = load_raw_dataset(data_path, dataset_name, split)\n",
        "    print(f\"Loaded {len(raw_dataset)} examples from {split} split.\")\n",
        "    prompt_template = load_prompt_templates(dataset_name)\n",
        "    outputs = []\n",
        "    for example in tqdm(raw_dataset):\n",
        "        # create prompt\n",
        "        try:\n",
        "            full_prompt = prompt_folio(example, prompt_template)\n",
        "            #inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\")#.to(device)\n",
        "\n",
        "            #temp = model.generate(inputs, temperature=0.01, do_sample=True, max_length=50)\n",
        "            inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "            temp = model.generate(inputs,temperature=0.01,do_sample = True)\n",
        "\n",
        "            output = tokenizer.decode(temp[0], skip_special_tokens=True)\n",
        "            # output = llm(full_prompt)\n",
        "            # print(full_prompt)\n",
        "            programs = [output]\n",
        "\n",
        "            # create output\n",
        "            output = {'id': example['id'],\n",
        "                    'context': example['context'],\n",
        "                    'question': example['question'],\n",
        "                    'answer': example['answer'],\n",
        "                    'options': example['options'],\n",
        "                    'raw_logic_programs': programs}\n",
        "            outputs.append(output)\n",
        "        except:\n",
        "            print('Error in generating logic programs for example: ', example['id'])\n",
        "\n",
        "    # save outputs\n",
        "    with open(os.path.join(save_path, f'{dataset_name}_{split}.json'), 'w') as f:\n",
        "        json.dump(outputs, f, indent=2, ensure_ascii=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAa9qAa66Vma"
      },
      "source": [
        "### backup gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHes6KeSfTaN"
      },
      "source": [
        "### fol parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilVII-rm1ENI"
      },
      "source": [
        "### when you want to run the followeing change the directroy to logicllm/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faJ9rNxZ63Dn"
      },
      "outputs": [],
      "source": [
        "os.chdir('models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uhg7qw4ZEPC"
      },
      "outputs": [],
      "source": [
        "%run fol_parser.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfC_mqijZO-b"
      },
      "outputs": [],
      "source": [
        "%run Formula.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RaSjmdTYtBf"
      },
      "outputs": [],
      "source": [
        "%run fol_prover9_parser.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwJcbMSLc8LV"
      },
      "outputs": [],
      "source": [
        "%run prover9_solver.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75xkgHb3Fa_v"
      },
      "outputs": [],
      "source": [
        "%run code_translator.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc29ycDeFkRE"
      },
      "outputs": [],
      "source": [
        "%run sat_problem_solver.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UV8Q-M99bSg"
      },
      "outputs": [],
      "source": [
        "%run backup_answer_generation.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL8HkDam63Dp"
      },
      "source": [
        "### logic inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5GkcV9yeFb1"
      },
      "outputs": [],
      "source": [
        "!python logic_inference.py \\\n",
        "    --dataset_name LogicalDeduction \\\n",
        "    --split dev \\\n",
        "    --backup_strategy random \\\n",
        "    --backup_LLM_result_path ../baselines/results/LogicalDeduction_CoT_dev.json \\\n",
        "    --save_path ../outputs/logic_inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDGy4k9a63Dp"
      },
      "source": [
        "### self refinement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNpdj5ZOxhzE"
      },
      "outputs": [],
      "source": [
        "!python self_refinement.py \\\n",
        "    --dataset_name FOLIO \\\n",
        "    --split dev \\\n",
        "    --backup_strategy LLM \\\n",
        "    --backup_LLM_result_path ../baselines/results/FOLIO_CoT_dev.json\\\n",
        "    --maximum_rounds 3"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}